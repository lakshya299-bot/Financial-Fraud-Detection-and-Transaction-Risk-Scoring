import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# -------------------- Load data --------------------
train_identity = pd.read_csv(
    '/Users/lakshyasmac/Desktop/Fraud detection pipeline/ieee-fraud-detection/train_identity.csv'
)
train_transaction = pd.read_csv(
    '/Users/lakshyasmac/Desktop/Fraud detection pipeline/ieee-fraud-detection/train_transaction.csv'
)

# -------------------- Merge --------------------
train = pd.merge(
    train_transaction,
    train_identity,
    on='TransactionID',
    how='left'
)

y = train["isFraud"]

# -------------------- Drop sparse columns --------------------
cols_to_remove_85 = [
    "D13","D14","D12","id_03","id_04","D6","id_33",
    "id_09","D8","D9","id_10","id_30","id_32","id_34",
    "id_14","V156","V161","V158","V162","V163"
]
train.drop(columns=cols_to_remove_85, inplace=True, errors="ignore")

# ==================== CHANGE 1 ====================
# Row-level missingness features (VERY IMPORTANT)
train["row_missing_ratio"] = train.isna().mean(axis=1)
train["row_missing_count"] = train.isna().sum(axis=1)

# ==================== ProductCD grouping ====================
train["ProductCD"] = train["ProductCD"].apply(
    lambda x: "W" if x == "W" else "OTHER"
)

# -------------------- TransactionDT features --------------------
train["Transaction_hour"] = (train["TransactionDT"] / 3600) % 24
train["Transaction_day"] = (train["TransactionDT"] / 86400).astype(int)
train["Transaction_week"] = (train["TransactionDT"] / 604800).astype(int)
train.drop(columns=["TransactionDT"], inplace=True)

# -------------------- TransactionAmt log --------------------
train["Log_TransactionAmt"] = np.log1p(train["TransactionAmt"])
train.drop(columns=["TransactionAmt"], inplace=True)
# ==================== Z-SCORE SCALING (Transaction Amount) ====================
amt_mean = train["Log_TransactionAmt"].mean()
amt_std = train["Log_TransactionAmt"].std()

train["Log_TransactionAmt_z"] = (
    train["Log_TransactionAmt"] - amt_mean
) / (amt_std + 1e-9)

# Drop original log feature to avoid redundancy
train.drop(columns=["Log_TransactionAmt"], inplace=True)

# -------------------- Features --------------------
X = train.drop(columns=["isFraud", "TransactionID"])

# -------------------- Train-validation split --------------------
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# -------------------- Categorical features --------------------
cat_features = [
    "ProductCD","card4","card6","P_emaildomain","R_emaildomain",
    "M1","M2","M3","M4","M5","M6","M7","M8","M9",
    "id_12","id_15","id_16","id_23","id_27","id_28",
    "id_29","id_31","id_35","id_36","id_37","id_38",
    "DeviceType","DeviceInfo"
]

num_features = [c for c in X_train.columns if c not in cat_features]

# -------------------- Pipelines --------------------
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA

num_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    # ==================== CHANGE 2 ====================
    ("pca", PCA(n_components=0.98, random_state=42))
])

cat_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", num_pipeline, num_features),
    ("cat", cat_pipeline, cat_features)
])

# ==================== CHANGE 3 ====================
# FIXED XGBOOST SETTINGS FOR FRAUD
from xgboost import XGBClassifier

scale_pos_weight = ((y_train == 0).sum() / (y_train == 1).sum()) * 1.5

from catboost import CatBoostClassifier

model = CatBoostClassifier(
    iterations=600,
    depth=8,
    learning_rate=0.05,
    loss_function="Logloss",
    eval_metric="AUC",
    auto_class_weights="Balanced",  # VERY IMPORTANT
    random_seed=42,
    verbose=100
)
# -------------------- Full pipeline --------------------
clf = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", model)
])

# -------------------- Train --------------------
clf.fit(X_train, y_train)

# -------------------- Evaluate --------------------
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve

y_val_proba = clf.predict_proba(X_val)[:, 1]

print("Validation ROC-AUC:", roc_auc_score(y_val, y_val_proba))

# Default high threshold (precision-focused)
print("\nThreshold 0.85")
print(classification_report(y_val, (y_val_proba >= 0.85).astype(int)))

# -------------------- Optimal threshold --------------------
precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)
f1 = 2 * precision * recall / (precision + recall + 1e-9)

best_idx = np.argmax(f1)
best_threshold = thresholds[best_idx]

print("\nBest threshold:", best_threshold)
print("Best F1:", f1[best_idx])

print("\nOptimal threshold report")
print(classification_report(
    y_val,
    (y_val_proba >= best_threshold).astype(int)
))
import joblib 
# Save the trained model
joblib.dump(clf, 'fraud_detection_model_catboost.pkl')